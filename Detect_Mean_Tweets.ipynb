{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detect_Mean_Tweets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjaminDKLuong/Colab_Notes/blob/master/Detect_Mean_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LULzBvzEYnCd",
        "colab_type": "text"
      },
      "source": [
        "# **DECTECT MEAN TWEETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN0xmrkoVpOv",
        "colab_type": "code",
        "outputId": "16e1b018-ca15-4ebc-fa7b-676044163f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "!git clone https://github.com/PacktPublishing/Real-World-Python-Deep-Learning-Projects.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Real-World-Python-Deep-Learning-Projects'...\n",
            "remote: Enumerating objects: 13503, done.\u001b[K\n",
            "remote: Total 13503 (delta 0), reused 0 (delta 0), pack-reused 13503\n",
            "Receiving objects: 100% (13503/13503), 178.97 MiB | 37.35 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "Checking out files: 100% (13472/13472), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTUNMwieY1jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change working directory to new location\n",
        "import os\n",
        "os.chdir(\"/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b348vXpG6or",
        "colab_type": "code",
        "outputId": "db5961aa-1dd7-4372-86fa-4822ee73b1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check current working directory\n",
        "%pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT8yw4JpHAbb",
        "colab_type": "code",
        "outputId": "669d3efc-fc75-438c-b051-64ff32c1764b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conf.py      mean_or_not.py\t prep.py       train.py\n",
            "data\t     models\t\t __pycache__   txt_sentoken\n",
            "data.tar.gz  poldata.README.2.0  stopwords.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka8Q90-fog8j",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHCaaapGokOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7ad7aa15-4bd5-45eb-a7ed-2cbce653c21f"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "\tif not os.path.isfile(file):\n",
        "\t\tprint(\"Download file... \" + file + \" ...\")\n",
        "\t\turlretrieve(url,file)\n",
        "print(\"File downloaded\")\n",
        "\n",
        "download('http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz','data.tar.gz')\n",
        "print(\"All the files are downloaded\")\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File downloaded\n",
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEdJf8MzqXJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "\n",
        "def uncompress_features_labels(dir):\n",
        "\n",
        "  if (dir.endswith(\"tar.gz\")):\n",
        "    tar = tarfile.open(dir, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "  elif (dir.endswith(\"tar\")):\n",
        "    tar = tarfile.open(dir, \"r:\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "\t\t\t\n",
        "uncompress_features_labels('data.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxwrrzmJ3xIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0603b304-9239-49c7-eb3d-800753522e55"
      },
      "source": [
        "'''import shutil\n",
        "import os\n",
        "\n",
        "source = '/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source/txt_sentoken/pos/'\n",
        "dest1 = '/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source/data/txt_sentoken/pos/'\n",
        "\n",
        "try:\n",
        "  os.mkdir(dest1)\n",
        "except:\n",
        "  print('dir exist')\n",
        "  \n",
        "files = os.listdir(source)\n",
        "\n",
        "for f in files:\n",
        "        shutil.move(source+f, dest1)'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dir exist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6mLsvJpBzEA",
        "colab_type": "text"
      },
      "source": [
        "## Import Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdqczL17b2FO",
        "colab_type": "code",
        "outputId": "cfe0a56f-cd8c-4670-d813-f99e4b992975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=set(stopwords.words('english'))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJnScbgFBLY_",
        "colab_type": "text"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAfePF8Qb2SO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "96eb974c-e518-412a-f136-80e1c938dad0"
      },
      "source": [
        "from os import listdir\n",
        "from os import path\n",
        "\n",
        "from stopwords import stopwords as exclude\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "def gen_x(xtext, tokenizer, max_len=None, for_training=False, ):\n",
        "    print(\"Let's tokenize!\")\n",
        "    # We \"fit\" our tokenizer on our training set.\n",
        "    # This is where unique numbers are generated for each word.\n",
        "    if for_training:\n",
        "        tokenizer.fit_on_texts(xtext)\n",
        "\n",
        "    # Encode words(tokens) as unique numbers.\n",
        "    encoded_xtext = tokenizer.texts_to_sequences(xtext)\n",
        "\n",
        "    # We're looking for the longest sentence\n",
        "    # in our training set.\n",
        "    # Then we will use it when we ran gen_x on test data.\n",
        "    # The key here is to have maximum lenght all the same trougout\n",
        "    # training and data sets.\n",
        "    if not max_len:\n",
        "        max_len = max([len(s.split()) for s in xtext])\n",
        "        tokenizer._max_padding_len=max_len\n",
        "\n",
        "    # We need to pad our encoded text to the maximum lenght\n",
        "    # for our embedding layer to work properly.\n",
        "    train_x = pad_sequences(encoded_xtext, maxlen=max_len, padding='post')\n",
        "\n",
        "    if for_training:\n",
        "        return train_x, max_len\n",
        "    return train_x\n",
        "\n",
        "def cleanup(w, clean_sw=True):\n",
        "    \"\"\"\n",
        "    Return a word if it's significant\n",
        "    and None if it can be filtered out.\n",
        "    clean_sw - should we filter out stop words?\n",
        "    \"\"\"\n",
        "    w=w.strip().lower()\n",
        "    if not w.isalpha():\n",
        "        return None\n",
        "    if clean_sw and w in exclude:\n",
        "        return None\n",
        "    if len(w) == 1:\n",
        "        return None\n",
        "    return w\n",
        "\n",
        "def clean(data, clean_sw):\n",
        "    \"\"\"\n",
        "    Remove unnecessary words and characters\n",
        "    from a data set.\n",
        "    data - a list of sentences to clean\n",
        "    clean_sw - should we filter out stop words?\n",
        "    \"\"\"\n",
        "    out=[]\n",
        "    for doc in data:\n",
        "        wout=[]\n",
        "        for w in doc.split():\n",
        "            w=cleanup(w, clean_sw)\n",
        "            if w == None:\n",
        "                continue\n",
        "            wout.append(w)\n",
        "        out.append(' '.join(wout))\n",
        "    return out\n",
        "\n",
        "def get_data(d='txt_sentoken', do_cleanup=True, filter_stopwords=True):\n",
        "    \"\"\"\n",
        "    Load all our data into memory,\n",
        "    split into training and data sets,\n",
        "    clean up and encode, so we can use it\n",
        "    with our neural network.\n",
        "    do_cleanup - should we remove insignificant characters and words?\n",
        "    filter_stopwords - should we remove common words?\n",
        "    \"\"\"\n",
        "    train_x=[]\n",
        "    train_y=[]\n",
        "\n",
        "    test_x=[]\n",
        "    test_y=[]\n",
        "\n",
        "    # First, load all of the data into train_x.\n",
        "    print('Loading data...')\n",
        "    for p in ['neg', 'pos']:\n",
        "        for filename in listdir(path.join(d,p)):\n",
        "            dfile = path.join(d,p,filename)\n",
        "            data=open(dfile).read()\n",
        "            train_x.append(data)\n",
        "\n",
        "    if do_cleanup:\n",
        "        print('Doing cleanup...')\n",
        "        ct=clean(train_x, filter_stopwords)\n",
        "    else:\n",
        "        ct=train_x\n",
        "\n",
        "    # Split our data set as training and test set.\n",
        "    # We have 1000 positive and 1000 negative reviews.\n",
        "    l=1000\n",
        "    # We split our data into 90% of data for training set\n",
        "    # and we leave 10% for testing.\n",
        "    trainl=int(l*0.90)\n",
        "    testl=int(l*0.10)\n",
        "\n",
        "    # First, spliting training set.\n",
        "    # Negative first.\n",
        "    train_x_neg=ct[0:trainl]\n",
        "    train_x_pos=ct[l:l+trainl]\n",
        "\n",
        "    # Generate approriate labels for negative data.\n",
        "    # 0 means negative, 1 positive.\n",
        "    train_y_neg=[ 0 for i in range(len(train_x_neg))]\n",
        "    train_y_pos=[ 1 for i in range(len(train_x_pos))]\n",
        "\n",
        "    # Put all of training splits together.\n",
        "    train_x=train_x_neg+train_x_pos\n",
        "    train_y=train_y_neg+train_y_pos\n",
        "\n",
        "    # Get the remining 10% of data as test set.\n",
        "    test_x_neg=ct[trainl:l]\n",
        "    test_x_pos=ct[l+trainl:]\n",
        "\n",
        "    test_y_neg=[ 0 for i in range(len(test_x_neg))]\n",
        "    test_y_pos=[ 1 for i in range(len(test_x_pos))]\n",
        "\n",
        "    test_x=test_x_neg+test_x_pos\n",
        "    test_y=test_y_neg+test_y_pos\n",
        "\n",
        "    # Create a new tokenizer, we will use it for both\n",
        "    # training and test data.\n",
        "    tokenizer=Tokenizer()\n",
        "    # Encode and pad our train and test data.\n",
        "    input_train_x = train_x\n",
        "    train_x, max_len = gen_x(train_x, tokenizer, for_training=True)\n",
        "    test_x = gen_x(test_x, tokenizer, max_len=max_len)\n",
        "\n",
        "    # Just show a sample of input text and encoded text.\n",
        "    print('Output from tokenizer:')\n",
        "    pprint(input_train_x[0][:50])\n",
        "    pprint(train_x[0][:9])\n",
        "    \n",
        "    for w in input_train_x[0][:50].replace(':','').split():\n",
        "        if w in tokenizer.word_index.keys():\n",
        "            print(w, '=', tokenizer.word_index[w])\n",
        "    print()\n",
        "\n",
        "\n",
        "    # Get a vocabulary size (a number of unique words).\n",
        "    # We will later have to use it for our Embedding layer.\n",
        "    inputs = len(tokenizer.word_index) + 1\n",
        "    print('Vocab size:')\n",
        "    print(inputs)\n",
        "    return train_x, train_y, test_x, test_y, inputs, max_len, tokenizer\n",
        "\n",
        "\n",
        "\n",
        "train_x, train_y, test_x, test_y, inputs, max_len, t=get_data()\n",
        "print('X[0]', train_x[0])\n",
        "print('Y[0]', train_y[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Doing cleanup...\n",
            "Let's tokenize!\n",
            "Let's tokenize!\n",
            "Output from tokenizer:\n",
            "'guess wild bachelor party gone really bad would br'\n",
            "array([ 470,  421, 6382,  614,  600,   21,   29,    9, 1537], dtype=int32)\n",
            "guess = 470\n",
            "wild = 421\n",
            "bachelor = 6382\n",
            "party = 614\n",
            "gone = 600\n",
            "really = 21\n",
            "bad = 29\n",
            "would = 9\n",
            "\n",
            "Vocab size:\n",
            "36202\n",
            "X[0] [ 470  421 6382 ...    0    0    0]\n",
            "Y[0] 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow3vZ5Jcb2O8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "80380770-a581-4f23-803f-c31dc1927464"
      },
      "source": [
        "print('len train_x', len(train_x))\n",
        "print('len train_y', len(train_y))\n",
        "print('len test_x', len(test_x))\n",
        "print('len test_y', len(test_y))\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len train_x 1800\n",
            "len train_y 1800\n",
            "len test_x 200\n",
            "len test_y 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b6-XHsKb2K0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d2ec9f2-9443-4fea-db72-0d17d4bc4b30"
      },
      "source": [
        "max_len"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXqIWRrcCJhb",
        "colab_type": "text"
      },
      "source": [
        "## Set Up Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qeI1Iib2Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import conf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Flatten\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import pandas\n",
        "import pickle\n",
        "\n",
        "from prep import get_data\n",
        "\n",
        "def get_text_cnn(inputs, max_length, dim=25):\n",
        "    \"\"\"\n",
        "    inputs - vocabulary size, a number of unique words in\n",
        "            our data set\n",
        "    max_lenght - the maximum number of words in our data set\n",
        "    dim - word embedding dimension, the lenght of word vector\n",
        "          that will be produced by this layer\n",
        "    \"\"\"\n",
        "    print('CNN: inputs: %d, word embeddings dimesions: %d, input_length: %d' % (inputs, dim, max_length))\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(inputs, dim, input_length=max_length))\n",
        "    # Extract feature maps/most common \"phrases\".\n",
        "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu', padding='same'))\n",
        "    # Pick up the \"best ones\", pooling=reducting.\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    # Just put everything together into one vector.\n",
        "    model.add(Flatten())\n",
        "    # This is the standard output for classification.\n",
        "    # It matches our two classes 0 and 1.\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "confs={'default': dict(model=get_text_cnn)}\n",
        "\n",
        "def train_model(name, train_x, train_y, epochs, batches, inputs, max_lenght, test_x, test_y):\n",
        "    \"\"\"\n",
        "    Compile and train model with choosen parameters.\n",
        "    \"\"\"\n",
        "    mparams=confs[name]\n",
        "    model=mparams['model']\n",
        "    model=model(inputs, max_lenght)\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Fit model on training data, validate during training on test data.\n",
        "    model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=epochs, batch_size=batches, verbose=2)\n",
        "    return model, name, mparams\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjnqtDWdb19k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3770
        },
        "outputId": "7221fcbc-e732-489f-f946-4ee432de42b6"
      },
      "source": [
        "\n",
        "# Getting our command line parameters\n",
        "name = 'default'\n",
        "epochs = 100\n",
        "batches= 32\n",
        "\n",
        "\n",
        "print('Train/Test Data lenght', len(train_x), len(test_x))\n",
        "\n",
        "model, name, mp = train_model(name, train_x, train_y, epochs, batches, inputs, max_length, test_x, test_y)\n",
        "\n",
        "# Save model to use for classification later on\n",
        "mname='models/model-%s-%d-%d' % (name, epochs, batches)\n",
        "model.save(mname+'.h5')\n",
        "\n",
        "with open(mname+'-tokenizer.pickle', 'wb') as ts:\n",
        "    pickle.dump(t, ts)\n",
        "title='%s (epochs=%d, batch_size=%d)' % (name, epochs, batches)\n",
        "\n",
        "# Test our model on both data that has been seen\n",
        "# (training data set) and unseen (test data set)\n",
        "print('Evaluation for %s' % title)\n",
        "\n",
        "loss, acc = model.evaluate(train_x, train_y, verbose=2)\n",
        "print('Train Accuracy: %.2f%%' % (acc*100))\n",
        "print()\n",
        "\n",
        "loss, acc = model.evaluate(test_x, test_y, verbose=2)\n",
        "print('Test Accuracy: %.2f%%' % (acc*100))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train/Test Data lenght 1800 200\n",
            "CNN: inputs: 36202, word embeddings dimesions: 25, input_length: 1299\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 1800 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            " - 4s - loss: 0.6888 - acc: 0.5356 - val_loss: 0.6858 - val_acc: 0.5450\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.6347 - acc: 0.6467 - val_loss: 0.6784 - val_acc: 0.5500\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4899 - acc: 0.8294 - val_loss: 0.6173 - val_acc: 0.6950\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.1995 - acc: 0.9917 - val_loss: 0.4804 - val_acc: 0.7700\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.0467 - acc: 0.9994 - val_loss: 0.4311 - val_acc: 0.7750\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.0155 - acc: 1.0000 - val_loss: 0.4160 - val_acc: 0.7900\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 0.4133 - val_acc: 0.7900\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4119 - val_acc: 0.7950\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.0033 - acc: 1.0000 - val_loss: 0.4167 - val_acc: 0.7900\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.4147 - val_acc: 0.7950\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4168 - val_acc: 0.7950\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4204 - val_acc: 0.7950\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4217 - val_acc: 0.7950\n",
            "Epoch 14/100\n",
            " - 0s - loss: 9.8450e-04 - acc: 1.0000 - val_loss: 0.4257 - val_acc: 0.7900\n",
            "Epoch 15/100\n",
            " - 0s - loss: 7.9980e-04 - acc: 1.0000 - val_loss: 0.4349 - val_acc: 0.8000\n",
            "Epoch 16/100\n",
            " - 0s - loss: 6.7437e-04 - acc: 1.0000 - val_loss: 0.4314 - val_acc: 0.7900\n",
            "Epoch 17/100\n",
            " - 0s - loss: 5.8054e-04 - acc: 1.0000 - val_loss: 0.4337 - val_acc: 0.7900\n",
            "Epoch 18/100\n",
            " - 0s - loss: 4.9380e-04 - acc: 1.0000 - val_loss: 0.4388 - val_acc: 0.8000\n",
            "Epoch 19/100\n",
            " - 0s - loss: 4.2659e-04 - acc: 1.0000 - val_loss: 0.4449 - val_acc: 0.8000\n",
            "Epoch 20/100\n",
            " - 0s - loss: 3.7670e-04 - acc: 1.0000 - val_loss: 0.4508 - val_acc: 0.8000\n",
            "Epoch 21/100\n",
            " - 0s - loss: 3.3302e-04 - acc: 1.0000 - val_loss: 0.4423 - val_acc: 0.7950\n",
            "Epoch 22/100\n",
            " - 0s - loss: 2.9515e-04 - acc: 1.0000 - val_loss: 0.4543 - val_acc: 0.8050\n",
            "Epoch 23/100\n",
            " - 0s - loss: 2.6426e-04 - acc: 1.0000 - val_loss: 0.4533 - val_acc: 0.8000\n",
            "Epoch 24/100\n",
            " - 0s - loss: 2.3870e-04 - acc: 1.0000 - val_loss: 0.4556 - val_acc: 0.8050\n",
            "Epoch 25/100\n",
            " - 0s - loss: 2.1559e-04 - acc: 1.0000 - val_loss: 0.4539 - val_acc: 0.8000\n",
            "Epoch 26/100\n",
            " - 0s - loss: 1.9650e-04 - acc: 1.0000 - val_loss: 0.4608 - val_acc: 0.8050\n",
            "Epoch 27/100\n",
            " - 0s - loss: 1.7891e-04 - acc: 1.0000 - val_loss: 0.4624 - val_acc: 0.8050\n",
            "Epoch 28/100\n",
            " - 0s - loss: 1.6513e-04 - acc: 1.0000 - val_loss: 0.4565 - val_acc: 0.8000\n",
            "Epoch 29/100\n",
            " - 0s - loss: 1.5204e-04 - acc: 1.0000 - val_loss: 0.4594 - val_acc: 0.8050\n",
            "Epoch 30/100\n",
            " - 0s - loss: 1.3843e-04 - acc: 1.0000 - val_loss: 0.4631 - val_acc: 0.8000\n",
            "Epoch 31/100\n",
            " - 0s - loss: 1.2813e-04 - acc: 1.0000 - val_loss: 0.4637 - val_acc: 0.8000\n",
            "Epoch 32/100\n",
            " - 0s - loss: 1.1832e-04 - acc: 1.0000 - val_loss: 0.4681 - val_acc: 0.8050\n",
            "Epoch 33/100\n",
            " - 0s - loss: 1.0927e-04 - acc: 1.0000 - val_loss: 0.4703 - val_acc: 0.8050\n",
            "Epoch 34/100\n",
            " - 0s - loss: 1.0169e-04 - acc: 1.0000 - val_loss: 0.4710 - val_acc: 0.8050\n",
            "Epoch 35/100\n",
            " - 0s - loss: 9.4659e-05 - acc: 1.0000 - val_loss: 0.4704 - val_acc: 0.8000\n",
            "Epoch 36/100\n",
            " - 0s - loss: 8.8197e-05 - acc: 1.0000 - val_loss: 0.4735 - val_acc: 0.8050\n",
            "Epoch 37/100\n",
            " - 0s - loss: 8.2287e-05 - acc: 1.0000 - val_loss: 0.4755 - val_acc: 0.8050\n",
            "Epoch 38/100\n",
            " - 0s - loss: 7.6973e-05 - acc: 1.0000 - val_loss: 0.4767 - val_acc: 0.8050\n",
            "Epoch 39/100\n",
            " - 0s - loss: 7.2044e-05 - acc: 1.0000 - val_loss: 0.4789 - val_acc: 0.8050\n",
            "Epoch 40/100\n",
            " - 0s - loss: 6.7571e-05 - acc: 1.0000 - val_loss: 0.4814 - val_acc: 0.8050\n",
            "Epoch 41/100\n",
            " - 0s - loss: 6.3510e-05 - acc: 1.0000 - val_loss: 0.4815 - val_acc: 0.8050\n",
            "Epoch 42/100\n",
            " - 0s - loss: 5.9885e-05 - acc: 1.0000 - val_loss: 0.4819 - val_acc: 0.8050\n",
            "Epoch 43/100\n",
            " - 0s - loss: 5.6152e-05 - acc: 1.0000 - val_loss: 0.4861 - val_acc: 0.8050\n",
            "Epoch 44/100\n",
            " - 0s - loss: 5.3106e-05 - acc: 1.0000 - val_loss: 0.4818 - val_acc: 0.8100\n",
            "Epoch 45/100\n",
            " - 0s - loss: 5.0033e-05 - acc: 1.0000 - val_loss: 0.4852 - val_acc: 0.8050\n",
            "Epoch 46/100\n",
            " - 0s - loss: 4.7276e-05 - acc: 1.0000 - val_loss: 0.4890 - val_acc: 0.8050\n",
            "Epoch 47/100\n",
            " - 0s - loss: 4.4602e-05 - acc: 1.0000 - val_loss: 0.4917 - val_acc: 0.8050\n",
            "Epoch 48/100\n",
            " - 0s - loss: 4.2233e-05 - acc: 1.0000 - val_loss: 0.4935 - val_acc: 0.8050\n",
            "Epoch 49/100\n",
            " - 0s - loss: 3.9972e-05 - acc: 1.0000 - val_loss: 0.4930 - val_acc: 0.8100\n",
            "Epoch 50/100\n",
            " - 0s - loss: 3.7851e-05 - acc: 1.0000 - val_loss: 0.4971 - val_acc: 0.8050\n",
            "Epoch 51/100\n",
            " - 0s - loss: 3.5872e-05 - acc: 1.0000 - val_loss: 0.4956 - val_acc: 0.8100\n",
            "Epoch 52/100\n",
            " - 0s - loss: 3.4069e-05 - acc: 1.0000 - val_loss: 0.4962 - val_acc: 0.8150\n",
            "Epoch 53/100\n",
            " - 0s - loss: 3.2378e-05 - acc: 1.0000 - val_loss: 0.4979 - val_acc: 0.8150\n",
            "Epoch 54/100\n",
            " - 0s - loss: 3.0806e-05 - acc: 1.0000 - val_loss: 0.4989 - val_acc: 0.8150\n",
            "Epoch 55/100\n",
            " - 0s - loss: 2.9298e-05 - acc: 1.0000 - val_loss: 0.5028 - val_acc: 0.8050\n",
            "Epoch 56/100\n",
            " - 0s - loss: 2.7841e-05 - acc: 1.0000 - val_loss: 0.5005 - val_acc: 0.8150\n",
            "Epoch 57/100\n",
            " - 0s - loss: 2.6634e-05 - acc: 1.0000 - val_loss: 0.5042 - val_acc: 0.8100\n",
            "Epoch 58/100\n",
            " - 0s - loss: 2.5233e-05 - acc: 1.0000 - val_loss: 0.5033 - val_acc: 0.8150\n",
            "Epoch 59/100\n",
            " - 0s - loss: 2.4103e-05 - acc: 1.0000 - val_loss: 0.5042 - val_acc: 0.8150\n",
            "Epoch 60/100\n",
            " - 0s - loss: 2.2974e-05 - acc: 1.0000 - val_loss: 0.5079 - val_acc: 0.8150\n",
            "Epoch 61/100\n",
            " - 0s - loss: 2.1894e-05 - acc: 1.0000 - val_loss: 0.5063 - val_acc: 0.8150\n",
            "Epoch 62/100\n",
            " - 0s - loss: 2.0933e-05 - acc: 1.0000 - val_loss: 0.5098 - val_acc: 0.8150\n",
            "Epoch 63/100\n",
            " - 0s - loss: 1.9968e-05 - acc: 1.0000 - val_loss: 0.5105 - val_acc: 0.8150\n",
            "Epoch 64/100\n",
            " - 0s - loss: 1.9085e-05 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.8150\n",
            "Epoch 65/100\n",
            " - 0s - loss: 1.8311e-05 - acc: 1.0000 - val_loss: 0.5104 - val_acc: 0.8150\n",
            "Epoch 66/100\n",
            " - 0s - loss: 1.7466e-05 - acc: 1.0000 - val_loss: 0.5144 - val_acc: 0.8200\n",
            "Epoch 67/100\n",
            " - 0s - loss: 1.6687e-05 - acc: 1.0000 - val_loss: 0.5160 - val_acc: 0.8200\n",
            "Epoch 68/100\n",
            " - 0s - loss: 1.6042e-05 - acc: 1.0000 - val_loss: 0.5164 - val_acc: 0.8200\n",
            "Epoch 69/100\n",
            " - 0s - loss: 1.5328e-05 - acc: 1.0000 - val_loss: 0.5170 - val_acc: 0.8200\n",
            "Epoch 70/100\n",
            " - 0s - loss: 1.4679e-05 - acc: 1.0000 - val_loss: 0.5202 - val_acc: 0.8200\n",
            "Epoch 71/100\n",
            " - 0s - loss: 1.4028e-05 - acc: 1.0000 - val_loss: 0.5217 - val_acc: 0.8200\n",
            "Epoch 72/100\n",
            " - 0s - loss: 1.3448e-05 - acc: 1.0000 - val_loss: 0.5208 - val_acc: 0.8200\n",
            "Epoch 73/100\n",
            " - 0s - loss: 1.2896e-05 - acc: 1.0000 - val_loss: 0.5237 - val_acc: 0.8200\n",
            "Epoch 74/100\n",
            " - 0s - loss: 1.2345e-05 - acc: 1.0000 - val_loss: 0.5244 - val_acc: 0.8200\n",
            "Epoch 75/100\n",
            " - 0s - loss: 1.1859e-05 - acc: 1.0000 - val_loss: 0.5235 - val_acc: 0.8200\n",
            "Epoch 76/100\n",
            " - 0s - loss: 1.1370e-05 - acc: 1.0000 - val_loss: 0.5249 - val_acc: 0.8200\n",
            "Epoch 77/100\n",
            " - 0s - loss: 1.0911e-05 - acc: 1.0000 - val_loss: 0.5262 - val_acc: 0.8200\n",
            "Epoch 78/100\n",
            " - 0s - loss: 1.0512e-05 - acc: 1.0000 - val_loss: 0.5275 - val_acc: 0.8200\n",
            "Epoch 79/100\n",
            " - 0s - loss: 1.0079e-05 - acc: 1.0000 - val_loss: 0.5307 - val_acc: 0.8200\n",
            "Epoch 80/100\n",
            " - 0s - loss: 9.6574e-06 - acc: 1.0000 - val_loss: 0.5307 - val_acc: 0.8200\n",
            "Epoch 81/100\n",
            " - 0s - loss: 9.2878e-06 - acc: 1.0000 - val_loss: 0.5311 - val_acc: 0.8200\n",
            "Epoch 82/100\n",
            " - 0s - loss: 8.9354e-06 - acc: 1.0000 - val_loss: 0.5306 - val_acc: 0.8250\n",
            "Epoch 83/100\n",
            " - 0s - loss: 8.5752e-06 - acc: 1.0000 - val_loss: 0.5341 - val_acc: 0.8250\n",
            "Epoch 84/100\n",
            " - 0s - loss: 8.2426e-06 - acc: 1.0000 - val_loss: 0.5366 - val_acc: 0.8200\n",
            "Epoch 85/100\n",
            " - 0s - loss: 7.9297e-06 - acc: 1.0000 - val_loss: 0.5368 - val_acc: 0.8250\n",
            "Epoch 86/100\n",
            " - 0s - loss: 7.6188e-06 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 0.8250\n",
            "Epoch 87/100\n",
            " - 0s - loss: 7.3448e-06 - acc: 1.0000 - val_loss: 0.5364 - val_acc: 0.8300\n",
            "Epoch 88/100\n",
            " - 0s - loss: 7.0574e-06 - acc: 1.0000 - val_loss: 0.5400 - val_acc: 0.8250\n",
            "Epoch 89/100\n",
            " - 0s - loss: 6.7898e-06 - acc: 1.0000 - val_loss: 0.5407 - val_acc: 0.8250\n",
            "Epoch 90/100\n",
            " - 0s - loss: 6.5372e-06 - acc: 1.0000 - val_loss: 0.5433 - val_acc: 0.8250\n",
            "Epoch 91/100\n",
            " - 0s - loss: 6.3308e-06 - acc: 1.0000 - val_loss: 0.5407 - val_acc: 0.8300\n",
            "Epoch 92/100\n",
            " - 0s - loss: 6.0613e-06 - acc: 1.0000 - val_loss: 0.5432 - val_acc: 0.8300\n",
            "Epoch 93/100\n",
            " - 0s - loss: 5.8293e-06 - acc: 1.0000 - val_loss: 0.5436 - val_acc: 0.8300\n",
            "Epoch 94/100\n",
            " - 0s - loss: 5.6239e-06 - acc: 1.0000 - val_loss: 0.5446 - val_acc: 0.8300\n",
            "Epoch 95/100\n",
            " - 0s - loss: 5.4123e-06 - acc: 1.0000 - val_loss: 0.5476 - val_acc: 0.8300\n",
            "Epoch 96/100\n",
            " - 0s - loss: 5.2127e-06 - acc: 1.0000 - val_loss: 0.5480 - val_acc: 0.8300\n",
            "Epoch 97/100\n",
            " - 0s - loss: 5.0267e-06 - acc: 1.0000 - val_loss: 0.5472 - val_acc: 0.8300\n",
            "Epoch 98/100\n",
            " - 0s - loss: 4.8599e-06 - acc: 1.0000 - val_loss: 0.5487 - val_acc: 0.8300\n",
            "Epoch 99/100\n",
            " - 0s - loss: 4.6668e-06 - acc: 1.0000 - val_loss: 0.5504 - val_acc: 0.8300\n",
            "Epoch 100/100\n",
            " - 0s - loss: 4.5068e-06 - acc: 1.0000 - val_loss: 0.5549 - val_acc: 0.8250\n",
            "Evaluation for default (epochs=100, batch_size=32)\n",
            "Train Accuracy: 100.00%\n",
            "\n",
            "Test Accuracy: 82.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oYlir2YVpIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJb_H_kfwsb_",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik_JlGuSHAG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ebcd185-e702-4e32-a376-ec77675fa201"
      },
      "source": [
        "tweet_original = input('Enter a tweet:  ')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a tweet:  im not happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0jjD0uDVo8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "27e4e3ba-eff0-42e2-c961-c7940b69f966"
      },
      "source": [
        "# Negative = 0\n",
        "# Positive = 1\n",
        "\n",
        "tweet = clean([tweet_original],True)\n",
        "print(tweet)\n",
        "print()\n",
        "\n",
        "test_tweet = gen_x(tweet, t, max_len=max_len)\n",
        "print(test_tweet)\n",
        "print()\n",
        "\n",
        "pred=model.predict_classes(test_tweet)\n",
        "pred=pred[0][0]\n",
        "print('Predict pos or neg: ',pred)\n",
        "\n",
        "prob=model.predict_proba(test_tweet)\n",
        "prob=prob[0][0]\n",
        "print('Probability:',prob)\n",
        "print()\n",
        "\n",
        "print('%s -%smean (%.2f%%)' % (tweet_original.rstrip(), (' ' if pred==0 else ' not '),(1-prob)*100))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['im happy']\n",
            "\n",
            "Let's tokenize!\n",
            "[[16155   573     0 ...     0     0     0]]\n",
            "\n",
            "Predict pos or neg:  0\n",
            "Probability: 0.020284083\n",
            "\n",
            "im not happy - mean (97.97%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HIxc3ZCi8dG",
        "colab_type": "text"
      },
      "source": [
        "## I need to fix the model.  It doesnt work as expected.  The reason maybe because of cleaning tweets.  You can try \"I'm happy\" and \"I'm not happy\" as inputs.  After cleaning the tweets, we get \"im happy\" for both cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpgTpIQZjZ5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}