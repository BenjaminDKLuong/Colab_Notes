{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detect_Mean_Tweets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjaminDKLuong/Colab_Notes/blob/master/Detect_Mean_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LULzBvzEYnCd",
        "colab_type": "text"
      },
      "source": [
        "# **DECTECT MEAN TWEETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1b7k0_H-wtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN0xmrkoVpOv",
        "colab_type": "code",
        "outputId": "04ad1794-96aa-4900-e43a-631aadc187fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/PacktPublishing/Real-World-Python-Deep-Learning-Projects.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Real-World-Python-Deep-Learning-Projects' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTUNMwieY1jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change working directory to new location\n",
        "import os\n",
        "os.chdir(\"/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b348vXpG6or",
        "colab_type": "code",
        "outputId": "134ae5cb-fad3-421a-8b10-dbc9eaba9e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check current working directory\n",
        "%pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT8yw4JpHAbb",
        "colab_type": "code",
        "outputId": "971768f3-1e59-4877-e955-0a5cc4d47c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conf.py\t\tmodels\t\t    Real-World-Python-Deep-Learning-Projects\n",
            "data\t\tpoldata.README.2.0  stopwords.py\n",
            "data.tar.gz\tprep.py\t\t    train.py\n",
            "mean_or_not.py\t__pycache__\t    txt_sentoken\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zT2L8mO_s0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4582c15-92c0-45bb-cd33-2b51fc1c8381"
      },
      "source": [
        "import shutil\n",
        "try:\n",
        "  shutil.rmtree('/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source/models')\n",
        "except:\n",
        "  print('fail to delete')\n",
        "else:\n",
        "  print('delete sucessfully')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fail to delete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka8Q90-fog8j",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHCaaapGokOG",
        "colab_type": "code",
        "outputId": "67129c63-144f-456f-f8ad-e89604e2e540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "\tif not os.path.isfile(file):\n",
        "\t\tprint(\"Download file... \" + file + \" ...\")\n",
        "\t\turlretrieve(url,file)\n",
        "print(\"File downloaded\")\n",
        "\n",
        "download('http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz','data.tar.gz')\n",
        "print(\"All the files are downloaded\")\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File downloaded\n",
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEdJf8MzqXJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "\n",
        "def uncompress_features_labels(dir):\n",
        "\n",
        "  if (dir.endswith(\"tar.gz\")):\n",
        "    tar = tarfile.open(dir, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "  elif (dir.endswith(\"tar\")):\n",
        "    tar = tarfile.open(dir, \"r:\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "\t\t\t\n",
        "uncompress_features_labels('data.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxwrrzmJ3xIT",
        "colab_type": "code",
        "outputId": "34551981-13d0-46cc-f9c0-266056e65d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''import shutil\n",
        "import os\n",
        "\n",
        "source = '/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source/txt_sentoken/pos/'\n",
        "dest1 = '/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source/data/txt_sentoken/pos/'\n",
        "\n",
        "try:\n",
        "  os.mkdir(dest1)\n",
        "except:\n",
        "  print('dir exist')\n",
        "  \n",
        "files = os.listdir(source)\n",
        "\n",
        "for f in files:\n",
        "        shutil.move(source+f, dest1)'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import shutil\\nimport os\\n\\nsource = '/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source/txt_sentoken/pos/'\\ndest1 = '/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source/data/txt_sentoken/pos/'\\n\\ntry:\\n  os.mkdir(dest1)\\nexcept:\\n  print('dir exist')\\n  \\nfiles = os.listdir(source)\\n\\nfor f in files:\\n        shutil.move(source+f, dest1)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6mLsvJpBzEA",
        "colab_type": "text"
      },
      "source": [
        "## Import Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdqczL17b2FO",
        "colab_type": "code",
        "outputId": "3a682db1-c777-4ea6-8b05-92ae5b93333b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=set(stopwords.words('english'))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJnScbgFBLY_",
        "colab_type": "text"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAfePF8Qb2SO",
        "colab_type": "code",
        "outputId": "a360975c-9907-4d93-d5db-dd9fc20039bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "from os import listdir\n",
        "from os import path\n",
        "\n",
        "from stopwords import stopwords as exclude\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "def gen_x(xtext, tokenizer, max_len=None, for_training=False, ):\n",
        "    print(\"Let's tokenize!\")\n",
        "    # We \"fit\" our tokenizer on our training set.\n",
        "    # This is where unique numbers are generated for each word.\n",
        "    if for_training:\n",
        "        tokenizer.fit_on_texts(xtext)\n",
        "\n",
        "    # Encode words(tokens) as unique numbers.\n",
        "    encoded_xtext = tokenizer.texts_to_sequences(xtext)\n",
        "\n",
        "    # We're looking for the longest sentence\n",
        "    # in our training set.\n",
        "    # Then we will use it when we ran gen_x on test data.\n",
        "    # The key here is to have maximum lenght all the same trougout\n",
        "    # training and data sets.\n",
        "    if not max_len:\n",
        "        max_len = max([len(s.split()) for s in xtext])\n",
        "        tokenizer._max_padding_len=max_len\n",
        "\n",
        "    # We need to pad our encoded text to the maximum lenght\n",
        "    # for our embedding layer to work properly.\n",
        "    train_x = pad_sequences(encoded_xtext, maxlen=max_len, padding='post')\n",
        "\n",
        "    if for_training:\n",
        "        return train_x, max_len\n",
        "    return train_x\n",
        "\n",
        "def cleanup(w, clean_sw=True):\n",
        "    \"\"\"\n",
        "    Return a word if it's significant\n",
        "    and None if it can be filtered out.\n",
        "    clean_sw - should we filter out stop words?\n",
        "    \"\"\"\n",
        "    w=w.strip().lower()\n",
        "    if not w.isalpha():\n",
        "        return None\n",
        "    if clean_sw and w in exclude:\n",
        "        return None\n",
        "    if len(w) == 1:\n",
        "        return None\n",
        "    return w\n",
        "\n",
        "def clean(data, clean_sw):\n",
        "    \"\"\"\n",
        "    Remove unnecessary words and characters\n",
        "    from a data set.\n",
        "    data - a list of sentences to clean\n",
        "    clean_sw - should we filter out stop words?\n",
        "    \"\"\"\n",
        "    out=[]\n",
        "    for doc in data:\n",
        "        wout=[]\n",
        "        for w in doc.split():\n",
        "            w=cleanup(w, clean_sw)\n",
        "            if w == None:\n",
        "                continue\n",
        "            wout.append(w)\n",
        "        out.append(' '.join(wout))\n",
        "    return out\n",
        "\n",
        "def get_data(d='txt_sentoken', do_cleanup=True, filter_stopwords=True):\n",
        "    \"\"\"\n",
        "    Load all our data into memory,\n",
        "    split into training and data sets,\n",
        "    clean up and encode, so we can use it\n",
        "    with our neural network.\n",
        "    do_cleanup - should we remove insignificant characters and words?\n",
        "    filter_stopwords - should we remove common words?\n",
        "    \"\"\"\n",
        "    train_x=[]\n",
        "    train_y=[]\n",
        "\n",
        "    test_x=[]\n",
        "    test_y=[]\n",
        "\n",
        "    # First, load all of the data into train_x.\n",
        "    print('Loading data...')\n",
        "    for p in ['neg', 'pos']:\n",
        "        for filename in listdir(path.join(d,p)):\n",
        "            dfile = path.join(d,p,filename)\n",
        "            data=open(dfile).read()\n",
        "            train_x.append(data)\n",
        "\n",
        "    if do_cleanup:\n",
        "        print('Doing cleanup...')\n",
        "        ct=clean(train_x, filter_stopwords)\n",
        "    else:\n",
        "        ct=train_x\n",
        "\n",
        "    # Split our data set as training and test set.\n",
        "    # We have 1000 positive and 1000 negative reviews.\n",
        "    l=1000\n",
        "    # We split our data into 90% of data for training set\n",
        "    # and we leave 10% for testing.\n",
        "    trainl=int(l*0.90)\n",
        "    testl=int(l*0.10)\n",
        "\n",
        "    # First, spliting training set.\n",
        "    # Negative first.\n",
        "    train_x_neg=ct[0:trainl]\n",
        "    train_x_pos=ct[l:l+trainl]\n",
        "\n",
        "    # Generate approriate labels for negative data.\n",
        "    # 0 means negative, 1 positive.\n",
        "    train_y_neg=[ 0 for i in range(len(train_x_neg))]\n",
        "    train_y_pos=[ 1 for i in range(len(train_x_pos))]\n",
        "\n",
        "    # Put all of training splits together.\n",
        "    train_x=train_x_neg+train_x_pos\n",
        "    train_y=train_y_neg+train_y_pos\n",
        "\n",
        "    # Get the remining 10% of data as test set.\n",
        "    test_x_neg=ct[trainl:l]\n",
        "    test_x_pos=ct[l+trainl:]\n",
        "\n",
        "    test_y_neg=[ 0 for i in range(len(test_x_neg))]\n",
        "    test_y_pos=[ 1 for i in range(len(test_x_pos))]\n",
        "\n",
        "    test_x=test_x_neg+test_x_pos\n",
        "    test_y=test_y_neg+test_y_pos\n",
        "\n",
        "    # Create a new tokenizer, we will use it for both\n",
        "    # training and test data.\n",
        "    tokenizer=Tokenizer()\n",
        "    # Encode and pad our train and test data.\n",
        "    input_train_x = train_x\n",
        "    train_x, max_len = gen_x(train_x, tokenizer, for_training=True)\n",
        "    test_x = gen_x(test_x, tokenizer, max_len=max_len)\n",
        "\n",
        "    # Just show a sample of input text and encoded text.\n",
        "    print('Output from tokenizer:')\n",
        "    pprint(input_train_x[0][:50])\n",
        "    pprint(train_x[0][:9])\n",
        "    \n",
        "    for w in input_train_x[0][:50].replace(':','').split():\n",
        "        if w in tokenizer.word_index.keys():\n",
        "            print(w, '=', tokenizer.word_index[w])\n",
        "    print()\n",
        "\n",
        "\n",
        "    # Get a vocabulary size (a number of unique words).\n",
        "    # We will later have to use it for our Embedding layer.\n",
        "    inputs = len(tokenizer.word_index) + 1\n",
        "    print('Vocab size:')\n",
        "    print(inputs)\n",
        "    return train_x, train_y, test_x, test_y, inputs, max_len, tokenizer\n",
        "\n",
        "\n",
        "\n",
        "train_x, train_y, test_x, test_y, inputs, max_len, t=get_data()\n",
        "print('X[0]', train_x[0])\n",
        "print('Y[0]', train_y[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Doing cleanup...\n",
            "Let's tokenize!\n",
            "Let's tokenize!\n",
            "Output from tokenizer:\n",
            "'guess wild bachelor party gone really bad would br'\n",
            "array([ 470,  421, 6382,  614,  600,   21,   29,    9, 1537], dtype=int32)\n",
            "guess = 470\n",
            "wild = 421\n",
            "bachelor = 6382\n",
            "party = 614\n",
            "gone = 600\n",
            "really = 21\n",
            "bad = 29\n",
            "would = 9\n",
            "\n",
            "Vocab size:\n",
            "36202\n",
            "X[0] [ 470  421 6382 ...    0    0    0]\n",
            "Y[0] 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow3vZ5Jcb2O8",
        "colab_type": "code",
        "outputId": "c6d9c733-1f24-4be9-cc81-84caf927fb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print('len train_x', len(train_x))\n",
        "print('len train_y', len(train_y))\n",
        "print('len test_x', len(test_x))\n",
        "print('len test_y', len(test_y))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len train_x 1800\n",
            "len train_y 1800\n",
            "len test_x 200\n",
            "len test_y 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b6-XHsKb2K0",
        "colab_type": "code",
        "outputId": "fc21c278-4dc1-4849-e4de-d42b4f45b7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_len"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXqIWRrcCJhb",
        "colab_type": "text"
      },
      "source": [
        "## Set Up Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qeI1Iib2Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import conf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Flatten\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import pandas\n",
        "import pickle\n",
        "\n",
        "from prep import get_data\n",
        "\n",
        "def get_text_cnn(inputs, max_length, dim=25):\n",
        "    \"\"\"\n",
        "    inputs - vocabulary size, a number of unique words in\n",
        "            our data set\n",
        "    max_lenght - the maximum number of words in our data set\n",
        "    dim - word embedding dimension, the lenght of word vector\n",
        "          that will be produced by this layer\n",
        "    \"\"\"\n",
        "    print('CNN: inputs: %d, word embeddings dimesions: %d, input_length: %d' % (inputs, dim, max_length))\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(inputs, dim, input_length=max_length))\n",
        "    # Extract feature maps/most common \"phrases\".\n",
        "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu', padding='same'))\n",
        "    # Pick up the \"best ones\", pooling=reducting.\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    # Just put everything together into one vector.\n",
        "    model.add(Flatten())\n",
        "    # This is the standard output for classification.\n",
        "    # It matches our two classes 0 and 1.\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "confs={'default': dict(model=get_text_cnn)}\n",
        "\n",
        "def train_model(name, train_x, train_y, epochs, batches, inputs, max_lenght, test_x, test_y):\n",
        "    \"\"\"\n",
        "    Compile and train model with choosen parameters.\n",
        "    \"\"\"\n",
        "    mparams=confs[name]\n",
        "    model=mparams['model']\n",
        "    model=model(inputs, max_lenght)\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Fit model on training data, validate during training on test data.\n",
        "    model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=epochs, batch_size=batches, verbose=2)\n",
        "    return model, name, mparams\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFJ9HgTWDOmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57797a00-09c4-4868-eba2-66b09af177ca"
      },
      "source": [
        "dirName = 'models'\n",
        "try:\n",
        "\tos.mkdir(dirName)\n",
        "except:\n",
        "  print('can not create models folder')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "can not create models folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjnqtDWdb19k",
        "colab_type": "code",
        "outputId": "7a957de1-4bf4-457e-c4b2-7551d92d0156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "\n",
        "# Getting our command line parameters\n",
        "name = 'default'\n",
        "epochs = 15\n",
        "batches= 32\n",
        "\n",
        "\n",
        "print('Train/Test Data lenght', len(train_x), len(test_x))\n",
        "\n",
        "model, name, mp = train_model(name, train_x, train_y, epochs, batches, inputs, max_len , test_x, test_y)\n",
        "\n",
        "# Save model to use for classification later on\n",
        "mname='models/model-%s-%d-%d' % (name, epochs, batches)\n",
        "model.save(mname+'.h5')\n",
        "\n",
        "with open(mname+'-tokenizer.pickle', 'wb') as ts:\n",
        "    pickle.dump(t, ts)\n",
        "title='%s (epochs=%d, batch_size=%d)' % (name, epochs, batches)\n",
        "\n",
        "# Test our model on both data that has been seen\n",
        "# (training data set) and unseen (test data set)\n",
        "print('Evaluation for %s' % title)\n",
        "\n",
        "loss, acc = model.evaluate(train_x, train_y, verbose=2)\n",
        "print('Train Accuracy: %.2f%%' % (acc*100))\n",
        "print()\n",
        "\n",
        "loss, acc = model.evaluate(test_x, test_y, verbose=2)\n",
        "print('Test Accuracy: %.2f%%' % (acc*100))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train/Test Data lenght 1800 200\n",
            "CNN: inputs: 36202, word embeddings dimesions: 25, input_length: 1299\n",
            "Train on 1800 samples, validate on 200 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 0.6885 - acc: 0.5383 - val_loss: 0.6843 - val_acc: 0.5550\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.6371 - acc: 0.6483 - val_loss: 0.6757 - val_acc: 0.5550\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.4997 - acc: 0.8228 - val_loss: 0.6155 - val_acc: 0.6750\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.2093 - acc: 0.9856 - val_loss: 0.4551 - val_acc: 0.8050\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.0482 - acc: 1.0000 - val_loss: 0.3902 - val_acc: 0.8400\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.0156 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 0.8250\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.3672 - val_acc: 0.8400\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.0047 - acc: 1.0000 - val_loss: 0.3637 - val_acc: 0.8450\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3635 - val_acc: 0.8500\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.3638 - val_acc: 0.8450\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.3647 - val_acc: 0.8450\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.3652 - val_acc: 0.8450\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3671 - val_acc: 0.8500\n",
            "Epoch 14/15\n",
            " - 0s - loss: 9.9970e-04 - acc: 1.0000 - val_loss: 0.3684 - val_acc: 0.8400\n",
            "Epoch 15/15\n",
            " - 0s - loss: 8.4837e-04 - acc: 1.0000 - val_loss: 0.3690 - val_acc: 0.8400\n",
            "Evaluation for default (epochs=15, batch_size=32)\n",
            "Train Accuracy: 100.00%\n",
            "\n",
            "Test Accuracy: 84.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJb_H_kfwsb_",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ZapwWI6Ax_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "53bcefb7-35d2-45ee-fc0d-5e349c7b7da5"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# looking the training above, I choose epochs based on loss and val_loss\n",
        "name = 'default'\n",
        "epochs = 15\n",
        "batches= 32\n",
        "\n",
        "\n",
        "mname='models/model-%s-%d-%d' % (name, epochs, batches)\n",
        "\n",
        "model_file=mname+'.h5'\n",
        "tokenizer_file=mname+'-tokenizer.pickle'\n",
        "\n",
        "# Loading the model.\n",
        "if os.path.exists(model_file):\n",
        "    model=load_model(model_file)\n",
        "    print('Model loaded!')\n",
        "else:\n",
        "    print(\"Can't find %s model, train it first using 'train.py %s %d %d'\" % (mname, name, epochs, batches))\n",
        "\n",
        "    \n",
        "if os.path.exists(tokenizer_file):\n",
        "    tokenizer=pickle.load(open( mname+'-tokenizer.pickle', \"rb\" ))\n",
        "    print('Tokenizer loaded!')\n",
        "else:\n",
        "    print(\"Can't find tokenizer for %s model, train it first using 'train.py %s %d %d'\" % (mname, name, epochs, batches))\n",
        "# Get the tweet."
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded!\n",
            "Tokenizer loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik_JlGuSHAG1",
        "colab_type": "code",
        "outputId": "1d43cbf1-1eb5-4f0c-ebd1-3068b6f7fedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweet_original = input('Enter a tweet:  ')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a tweet:  I hate you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0jjD0uDVo8_",
        "colab_type": "code",
        "outputId": "aae873ce-3e5f-470e-bb02-3c9e2821f16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# Negative = 0\n",
        "# Positive = 1\n",
        "\n",
        "tweet = clean([tweet_original],True)\n",
        "print(tweet)\n",
        "print()\n",
        "\n",
        "test_tweet = gen_x(tweet, t, max_len=max_len)\n",
        "print(test_tweet)\n",
        "print()\n",
        "\n",
        "pred=model.predict_classes(test_tweet)\n",
        "pred=pred[0][0]\n",
        "print('Predict: ',pred)\n",
        "\n",
        "prob=model.predict_proba(test_tweet)\n",
        "prob=prob[0][0]\n",
        "print('Probability:',prob)\n",
        "print()\n",
        "\n",
        "print('%s -%smean(negative) (%.2f%%)' % (tweet_original.rstrip(), (' ' if pred==0 else ' not '),(1-prob)*100))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hate']\n",
            "\n",
            "Let's tokenize!\n",
            "[[899   0   0 ...   0   0   0]]\n",
            "\n",
            "Predict:  1\n",
            "Probability: 0.5148021\n",
            "\n",
            "I hate you - not mean(negative) (48.52%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HIxc3ZCi8dG",
        "colab_type": "text"
      },
      "source": [
        "## I need to fix the model.  It doesnt work as expected.  The reason maybe because of cleaning tweets.  You can try \"I'm happy\" and \"I'm not happy\" as inputs.  After cleaning the tweets, we get \"im happy\" for both cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpgTpIQZjZ5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx2BWkFW8-mF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}