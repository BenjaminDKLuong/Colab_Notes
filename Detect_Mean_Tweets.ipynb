{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detect_Mean_Tweets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjaminDKLuong/Colab_Notes/blob/master/Detect_Mean_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LULzBvzEYnCd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **DECTECT MEAN TWEETS**"
      ]
    },
    {
      "metadata": {
        "id": "mN0xmrkoVpOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "0516e36d-23c8-4c42-ad41-c1d372015500"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PacktPublishing/Real-World-Python-Deep-Learning-Projects.git"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Real-World-Python-Deep-Learning-Projects'...\n",
            "remote: Enumerating objects: 13503, done.\u001b[K\n",
            "remote: Total 13503 (delta 0), reused 0 (delta 0), pack-reused 13503\u001b[K\n",
            "Receiving objects: 100% (13503/13503), 178.97 MiB | 31.05 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "Checking out files: 100% (13472/13472), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UTUNMwieY1jM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# change working directory to new location\n",
        "import os\n",
        "os.chdir(\"/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_b348vXpG6or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5217ba0f-9940-4c2f-f7f7-3806189f96d1"
      },
      "cell_type": "code",
      "source": [
        "# check current working directory\n",
        "%pwd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Real-World-Python-Deep-Learning-Projects/Section 3 Code/source'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "WT8yw4JpHAbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "81c95edb-75f7-4e1b-d960-76722fead106"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conf.py  mean_or_not.py  prep.py      stopwords.py\n",
            "data\t models\t\t __pycache__  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P6mLsvJpBzEA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Stopwords"
      ]
    },
    {
      "metadata": {
        "id": "mdqczL17b2FO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=set(stopwords.words('english'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJnScbgFBLY_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Process Data"
      ]
    },
    {
      "metadata": {
        "id": "tAfePF8Qb2SO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os import path\n",
        "\n",
        "from stopwords import stopwords as exclude\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "def gen_x(xtext, tokenizer, max_len=None, for_training=False, ):\n",
        "    print(\"Let's tokenize!\")\n",
        "    # We \"fit\" our tokenizer on our training set.\n",
        "    # This is where unique numbers are generated for each word.\n",
        "    if for_training:\n",
        "        tokenizer.fit_on_texts(xtext)\n",
        "\n",
        "    # Encode words(tokens) as unique numbers.\n",
        "    encoded_xtext = tokenizer.texts_to_sequences(xtext)\n",
        "\n",
        "    # We're looking for the longest sentence\n",
        "    # in our training set.\n",
        "    # Then we will use it when we ran gen_x on test data.\n",
        "    # The key here is to have maximum lenght all the same trougout\n",
        "    # training and data sets.\n",
        "    if not max_len:\n",
        "        max_len = max([len(s.split()) for s in xtext])\n",
        "        tokenizer._max_padding_len=max_len\n",
        "\n",
        "    # We need to pad our encoded text to the maximum lenght\n",
        "    # for our embedding layer to work properly.\n",
        "    train_x = pad_sequences(encoded_xtext, maxlen=max_len, padding='post')\n",
        "\n",
        "    if for_training:\n",
        "        return train_x, max_len\n",
        "    return train_x\n",
        "\n",
        "def cleanup(w, clean_sw=True):\n",
        "    \"\"\"\n",
        "    Return a word if it's significant\n",
        "    and None if it can be filtered out.\n",
        "    clean_sw - should we filter out stop words?\n",
        "    \"\"\"\n",
        "    w=w.strip().lower()\n",
        "    if not w.isalpha():\n",
        "        return None\n",
        "    if clean_sw and w in exclude:\n",
        "        return None\n",
        "    if len(w) == 1:\n",
        "        return None\n",
        "    return w\n",
        "\n",
        "def clean(data, clean_sw):\n",
        "    \"\"\"\n",
        "    Remove unnecessary words and characters\n",
        "    from a data set.\n",
        "    data - a list of sentences to clean\n",
        "    clean_sw - should we filter out stop words?\n",
        "    \"\"\"\n",
        "    out=[]\n",
        "    for doc in data:\n",
        "        wout=[]\n",
        "        for w in doc.split():\n",
        "            w=cleanup(w, clean_sw)\n",
        "            if w == None:\n",
        "                continue\n",
        "            wout.append(w)\n",
        "        out.append(' '.join(wout))\n",
        "    return out\n",
        "\n",
        "def get_data(d='data/txt_sentoken', do_cleanup=True, filter_stopwords=True):\n",
        "    \"\"\"\n",
        "    Load all our data into memory,\n",
        "    split into training and data sets,\n",
        "    clean up and encode, so we can use it\n",
        "    with our neural network.\n",
        "    do_cleanup - should we remove insignificant characters and words?\n",
        "    filter_stopwords - should we remove common words?\n",
        "    \"\"\"\n",
        "    train_x=[]\n",
        "    train_y=[]\n",
        "\n",
        "    test_x=[]\n",
        "    test_y=[]\n",
        "\n",
        "    # First, load all of the data into train_x.\n",
        "    print('Loading data...')\n",
        "    for p in ['neg', 'pos']:\n",
        "        for filename in listdir(path.join(d,p)):\n",
        "            dfile = path.join(d,p,filename)\n",
        "            data=open(dfile).read()\n",
        "            train_x.append(data)\n",
        "\n",
        "    if do_cleanup:\n",
        "        print('Doing cleanup...')\n",
        "        ct=clean(train_x, filter_stopwords)\n",
        "    else:\n",
        "        ct=train_x\n",
        "\n",
        "    # Split our data set as training and test set.\n",
        "    # We have 1000 positive and 100 negative reviews.\n",
        "    l=1000\n",
        "    # We split our data into 90% of data for training set\n",
        "    # and we leave 10% for testing.\n",
        "    trainl=int(l*0.90)\n",
        "    testl=int(l*0.10)\n",
        "\n",
        "    # First, spliting training set.\n",
        "    # Negative first.\n",
        "    train_x_neg=ct[0:trainl]\n",
        "    train_x_pos=ct[l:l+trainl]\n",
        "\n",
        "    # Generate approriate labels for negative data.\n",
        "    # 0 means negative, 1 positive.\n",
        "    train_y_neg=[ 0 for i in range(len(train_x_neg))]\n",
        "    train_y_pos=[ 1 for i in range(len(train_x_pos))]\n",
        "\n",
        "    # Put all of training splits together.\n",
        "    train_x=train_x_neg+train_x_pos\n",
        "    train_y=train_y_neg+train_y_pos\n",
        "\n",
        "    # Get the remining 10% of data as test set.\n",
        "    test_x_neg=ct[trainl:l]\n",
        "    test_x_pos=ct[l+trainl:]\n",
        "\n",
        "    test_y_neg=[ 0 for i in range(len(test_x_neg))]\n",
        "    test_y_pos=[ 1 for i in range(len(test_x_neg))]\n",
        "\n",
        "    test_x=test_x_neg+test_x_pos\n",
        "    test_y=test_y_neg+test_y_pos\n",
        "\n",
        "    # Create a new tokenizer, we will use it for both\n",
        "    # training and test data.\n",
        "    tokenizer=Tokenizer()\n",
        "    # Encode and pad our train and test data.\n",
        "    input_train_x=train_x\n",
        "    train_x, max_len=gen_x(train_x, tokenizer, for_training=True)\n",
        "    test_x=gen_x(test_x, tokenizer, max_len=max_len)\n",
        "\n",
        "    # Just show a sample of input text and encoded text.\n",
        "    print('Output from tokenizer:')\n",
        "    pprint(input_train_x[0][:50])\n",
        "    pprint(train_x[0][:9])\n",
        "    for w in input_train_x[0][:50].replace(':','').split():\n",
        "        if w in tokenizer.word_index.keys():\n",
        "            print(w, '=', tokenizer.word_index[w])\n",
        "    print()\n",
        "\n",
        "\n",
        "    # Get a vocabulary size (a number of unique words).\n",
        "    # We will later have to use it for our Embedding layer.\n",
        "    inputs = len(tokenizer.word_index) + 1\n",
        "    print('Vocab size:')\n",
        "    print(inputs)\n",
        "    return train_x, train_y, test_x, test_y, inputs, max_len, tokenizer\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_x, train_y, test_x, test_y, inputs, max_len, t=get_data()\n",
        "    print('X[0]', train_x[0])\n",
        "    print('Y[0]', train_y[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ow3vZ5Jcb2O8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7b6-XHsKb2K0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PJ6l0qJ5b2Hy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vXqIWRrcCJhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set Up Model"
      ]
    },
    {
      "metadata": {
        "id": "r-qeI1Iib2Bm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import conf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Flatten\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import pandas\n",
        "import pickle\n",
        "\n",
        "from prep import get_data\n",
        "\n",
        "def get_text_cnn(inputs, max_length, dim=25):\n",
        "    \"\"\"\n",
        "    input - vocabulary size, a number of unique words in\n",
        "            our data set\n",
        "    max_lenght - the maximum number of words in our data set\n",
        "    dim - word embedding dimension, the lenght of word vector\n",
        "          that will be produced by this layer\n",
        "    \"\"\"\n",
        "    print('CNN: inputs: %d, word embeddings dimesions: %d, input_length: %d' % (inputs, dim, max_length))\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(inputs, dim, input_length=max_length))\n",
        "    # Extract feature maps/most common \"phrases\".\n",
        "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu', padding='same'))\n",
        "    # Pick up the \"best ones\", pooling=reducting.\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    # Just put everything together into one vector.\n",
        "    model.add(Flatten())\n",
        "    # This is the standard output for classification.\n",
        "    # It matches our two classes 0 and 1.\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "confs={'default': dict(model=get_text_cnn)}\n",
        "\n",
        "def train_model(name, train_x, train_y, epochs, batches, inputs, max_lenght, test_x, test_y):\n",
        "    \"\"\"\n",
        "    Compile and train model with choosen parameters.\n",
        "    \"\"\"\n",
        "    mparams=confs[name]\n",
        "    model=mparams['model']\n",
        "    model=model(inputs, max_lenght)\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Fit model on training data, validate during training on test data.\n",
        "    model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=epochs, batch_size=batches, verbose=2)\n",
        "    return model, name, mparams\n",
        "\n",
        "def get_params(script='train.py'):\n",
        "    \"\"\"\n",
        "    Get command line parameters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        name, epochs, batches=sys.argv[1:4]\n",
        "    except ValueError:\n",
        "        print('Usage: %s model_name epochs batch_size' % sys.argv[0])\n",
        "        exit(1)\n",
        "    return name, int(epochs), int(batches)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Getting our command line parameters\n",
        "    name, epochs, batches=get_params()\n",
        "    train_x, train_y, test_x, test_y, inputs, max_length, t=get_data(do_cleanup=True, filter_stopwords=True)\n",
        "    print('Train/Test Data lenght', len(train_x), len(test_x))\n",
        "    model, name, mp =train_model(name, train_x, train_y, epochs, batches, inputs, max_length, test_x, test_y)\n",
        "    # Save model to use for classification later on\n",
        "    mname='models/model-%s-%d-%d' % (name, epochs, batches)\n",
        "    model.save(mname+'.h5')\n",
        "    with open(mname+'-tokenizer.pickle', 'wb') as ts:\n",
        "        pickle.dump(t, ts)\n",
        "    title='%s (epochs=%d, batch_size=%d)' % (name, epochs, batches)\n",
        "    # Test our model on both data that has been seen\n",
        "    # (training data set) and unseen (test data set)\n",
        "    print('Evaluation for %s' % title)\n",
        "    loss, acc = model.evaluate(train_x, train_y, verbose=2)\n",
        "    print('Train Accuracy: %.2f%%' % (acc*100))\n",
        "    loss, acc = model.evaluate(test_x, test_y, verbose=2)\n",
        "    print('Test Accuracy: %.2f%%' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BjnqtDWdb19k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7oYlir2YVpIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1b9AV7ynVpGI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJb_H_kfwsb_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ]
    },
    {
      "metadata": {
        "id": "p1VafW-LVpAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rom train import get_params, confs\n",
        "from prep import clean\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    name, epochs, batches=get_params()\n",
        "    model=confs[name]\n",
        "    mname='models/model-%s-%d-%d' % (name, epochs, batches)\n",
        "    model_file=mname+'.h5'\n",
        "    tokenizer_file=mname+'-tokenizer.pickle'\n",
        "    # Loading the model.\n",
        "    if os.path.exists(model_file):\n",
        "        model=load_model(model_file)\n",
        "        print('Model loaded!')\n",
        "    else:\n",
        "        print(\"Can't find %s model, train it first using 'train.py %s %d %d'\" % (mname, name, epochs, batches))\n",
        "    # Loading tokenizer.\n",
        "    # We need to use the same tokenizer that we've used\n",
        "    # for training and testing to get the same encoding\n",
        "    # for known words in our vocabulary and also in\n",
        "    # the word embedding that we've created during the training.\n",
        "    if os.path.exists(tokenizer_file):\n",
        "        tokenizer=pickle.load(open( mname+'-tokenizer.pickle', \"rb\" ))\n",
        "        print('Tokenizer loaded!')\n",
        "    else:\n",
        "        print(\"Can't find tokenizer for %s model, train it first using 'train.py %s %d %d'\" % (mname, name, epochs, batches))\n",
        "    # Get the tweet.\n",
        "    print(\"Type in one tweet per line and hit CTRT-D when you're done:\")\n",
        "    for tweet in sys.stdin.readlines():\n",
        "        # Cleanup the tweet before we use our model.\n",
        "        t=clean([tweet], True)\n",
        "        # Encode and pad our tweet with the same tokenizer\n",
        "        # that we've used for training and testing.\n",
        "        # We've set our own variable in\n",
        "        # tokenizer._max_padding_len on training to store\n",
        "        # informations about the maximum lenght of our encoded text.\n",
        "        t=tokenizer.texts_to_sequences(t)\n",
        "        t=pad_sequences(t, maxlen=tokenizer._max_padding_len, padding='post')\n",
        "        # Get one of a predicted classes\n",
        "        # In our case it's 0 for negative tweet and 1 for positive.\n",
        "        pc=model.predict_classes(t)\n",
        "        pc=pc[0][0]\n",
        "        # We can also can get the probablity of prediction been in a given class.\n",
        "        # By default we get the probablity of being in class no. 1 which in our\n",
        "        # case is probability of a tweet to be postive.\n",
        "        # We can get the probablity of tweet being mean just by calculating 1-prob.\n",
        "        prob=model.predict_proba(t)\n",
        "        prob=prob[0][0]\n",
        "        print('%s -%smean (%.2f%%)' % (tweet.rstrip(), (' ' if pc==0 else ' not '),(1-prob)*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J0jjD0uDVo8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ik_JlGuSHAG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6TVniSpHAER",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqE34HbaHAA1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0QVatci2G_-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWRO2pKcG_8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ISGRxL8qG_5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArXocxazG_1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoScrZZ5G_zn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8R998VRoG_wt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eoZc_AVsG_s5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5aZ8_ujhG_qU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1H45iYT6G_nm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}